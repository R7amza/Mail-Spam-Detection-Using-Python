{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323d3b4a",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">MIS 542 &ndash; Advanced Business Analytics</h1>\n",
    "<h1 style=\"text-align: center;\">Course Project - Spam Detection</h1>\n",
    "\n",
    "<h4> Team 7 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db08df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rs7am\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rs7am\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rs7am\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from dmba import classificationSummary\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94a050",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12fd80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mail_df = pd.read_csv('D:/MIS542/Project file.csv') \n",
    "mail_df = mail_df[['class', 'message']] #To have only wanted columns\n",
    "mail_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d494ffe",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab918c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid    4825\n",
      "spam      747\n",
      "Name: class, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\rs7am\\Anaconda\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='class'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=mail_df['class'].value_counts()\n",
    "print(x)\n",
    "sns.barplot(x.index,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bf60e",
   "metadata": {},
   "source": [
    "#### Function for lowering case,  removing of special characters, stop words, hyperlinks, numbers, and white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d74e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip() \n",
    "    text = re.compile('<.*?>').sub(' ', text)\n",
    "    text = re.compile('http:\\/\\/.*[\\r\\n]*').sub(' ', text)\n",
    "    text = re.compile('https:\\/\\/.*[\\r\\n]*').sub(' ', text) \n",
    "    text = re.compile('www.\\/\\/.*[\\r\\n]*').sub(' ', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'[^a-zA-Z0-9]',' ',text) \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text)  \n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753180c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "mail_df['clean_message'] = mail_df['message'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4eebc01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "      <th>clean_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>valid</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>go jurong point crazy available bugis n great world la e buffet cine get amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valid</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>valid</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>valid</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  \\\n",
       "0  valid   \n",
       "1  valid   \n",
       "2   spam   \n",
       "3  valid   \n",
       "4  valid   \n",
       "\n",
       "                                                                                                                                                       message  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                           clean_message  \n",
       "0                     go jurong point crazy available bugis n great world la e buffet cine get amore wat  \n",
       "1                                                                                ok lar joking wif u oni  \n",
       "2  free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply  \n",
       "3                                                                    u dun say early hor u c already say  \n",
       "4                                                                    nah think go usf life around though  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all rows in the dataframe                                                                  \n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_colwidth\", 199)\n",
    " \n",
    "# printing data frame\n",
    "mail_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b8db3",
   "metadata": {},
   "source": [
    "#### Text Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b3a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(mail_df['clean_message'])\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba9a124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract concepts using LSA ()\n",
    "\n",
    "svd = TruncatedSVD()\n",
    "normalizer = MinMaxScaler() # instead of Normalizer, we replaced it by MinMaxScaler to avoid negative values for NB model.\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d654f",
   "metadata": {},
   "source": [
    "# Analysis and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f881e",
   "metadata": {},
   "source": [
    "#### Data splitting and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a89dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_df.loc[mail_df['class'] == 'Valid', 'class',] = 1\n",
    "mail_df.loc[mail_df['class'] == 'spam', 'class',] = 0\n",
    "y = (mail_df['class'] == 0).astype(int)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(lsa_tfidf, y, \n",
    "                                                      test_size=0.4, \n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b966e8",
   "metadata": {},
   "source": [
    "### Oversampling spam class in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "340f376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2895, 1: 448})\n",
      "Counter({0: 2895, 1: 2895})\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# instantiating the random over sampler \n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# resampling X_train, y_train\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# class distribution\n",
    "print(Counter(y_train))\n",
    "# new class distribution \n",
    "print(Counter(y_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95608626",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcf698",
   "metadata": {},
   "source": [
    "#### Without Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b430c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.8660)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2895    0\n",
      "     1  448    0\n",
      "Confusion Matrix (Accuracy 0.8659)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1930    0\n",
      "     1  299    0\n"
     ]
    }
   ],
   "source": [
    "# run naive Bayes\n",
    "mail_nb = MultinomialNB(alpha=0.01)\n",
    "mail_nb.fit(X_train, y_train)\n",
    "\n",
    "# predict probabilities\n",
    "predProb_train = mail_nb.predict_proba(X_train)\n",
    "predProb_valid = mail_nb.predict_proba(X_valid)\n",
    "\n",
    "# predict class membership\n",
    "y_train_pred = mail_nb.predict(X_train)\n",
    "y_valid_pred = mail_nb.predict(X_valid)\n",
    "\n",
    "# training\n",
    "classificationSummary(y_train, y_train_pred)\n",
    "# validation\n",
    "classificationSummary(y_valid, y_valid_pred)\n",
    "\n",
    "l = [0,0]\n",
    "NB_table1 = pd.concat([pd.crosstab(y_valid, y_valid_pred), pd.DataFrame(l)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb01b8",
   "metadata": {},
   "source": [
    "#### With Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ffa276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.7503)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2408  487\n",
      "     1  959 1936\n",
      "Confusion Matrix (Accuracy 0.8201)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1614  316\n",
      "     1   85  214\n"
     ]
    }
   ],
   "source": [
    "# run naive Bayes\n",
    "mail_nb1 = MultinomialNB(alpha=0.01)\n",
    "mail_nb1.fit(X_ros, y_ros)\n",
    "\n",
    "# predict probabilities\n",
    "predProb_train = mail_nb1.predict_proba(X_ros)\n",
    "predProb_valid = mail_nb1.predict_proba(X_valid)\n",
    "\n",
    "# predict class membership\n",
    "y_train_pred = mail_nb1.predict(X_ros)\n",
    "y_valid_pred = mail_nb1.predict(X_valid)\n",
    "\n",
    "# training\n",
    "classificationSummary(y_ros, y_train_pred)\n",
    "# validation\n",
    "classificationSummary(y_valid, y_valid_pred)\n",
    "\n",
    "NB_table2 = pd.crosstab(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5937b6",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d21e9",
   "metadata": {},
   "source": [
    "#### Without Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "885e2e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9728)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2868   27\n",
      "     1   64  384\n",
      "Confusion Matrix (Accuracy 0.9560)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1888   42\n",
      "     1   56  243\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict testing set\n",
    "y_pred = knn.predict(X_valid)\n",
    "\n",
    "# training\n",
    "classificationSummary(y_train, knn.predict(X_train))\n",
    "# validation\n",
    "classificationSummary(y_valid, y_pred)\n",
    "\n",
    "KNN_table1 = pd.crosstab(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fffe42c",
   "metadata": {},
   "source": [
    "#### With Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f4da6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9848)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2807   88\n",
      "     1    0 2895\n",
      "Confusion Matrix (Accuracy 0.9305)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1822  108\n",
      "     1   47  252\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "knn1 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn1.fit(X_ros, y_ros)\n",
    "\n",
    "#Predict testing set\n",
    "y_pred = knn1.predict(X_valid)\n",
    "\n",
    "# training\n",
    "classificationSummary(y_ros, knn1.predict(X_ros))\n",
    "# validation\n",
    "classificationSummary(y_valid, y_pred)\n",
    "\n",
    "KNN_table2 = pd.crosstab(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa67fe6",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a14cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(matrix, model_name):\n",
    "    tn = matrix.iloc[0,0] \n",
    "    tp = matrix.iloc[1,1] \n",
    "    fn = matrix.iloc[1,0] \n",
    "    fp = matrix.iloc[0,1] \n",
    "    tap = fn+tp\n",
    "    tan = tn+fp\n",
    "    tpn = tn+fn\n",
    "    tpp = fp+tp\n",
    "    precision = tp / tpp\n",
    "    recall = tp / tap\n",
    "    total = tn+tp+fn+fp\n",
    "    \n",
    "    \n",
    "    data = [\n",
    "        round((tp+tn)/(total),4),\n",
    "        round(1-((tp+tn)/(total)),4),\n",
    "        round(tp /(tap),4),\n",
    "        round(tn /(tan),4),\n",
    "        round(precision,4),\n",
    "                                    ]\n",
    "    \n",
    "    return(pd.DataFrame(data, columns=[model_name], \n",
    "                        index = ['Accuracy','Error Rate','Sensitivity','Specificity','Precision']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c3f5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-f5c1f2b71443>:10: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = tp / tpp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NB</th>\n",
       "      <th>NB w OS</th>\n",
       "      <th>KNN</th>\n",
       "      <th>KNN w OS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.8659</td>\n",
       "      <td>0.8201</td>\n",
       "      <td>0.9560</td>\n",
       "      <td>0.9305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error Rate</th>\n",
       "      <td>0.1341</td>\n",
       "      <td>0.1799</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7157</td>\n",
       "      <td>0.8127</td>\n",
       "      <td>0.8428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8363</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.9440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4038</td>\n",
       "      <td>0.8526</td>\n",
       "      <td>0.7000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 NB  NB w OS     KNN  KNN w OS\n",
       "Accuracy     0.8659   0.8201  0.9560    0.9305\n",
       "Error Rate   0.1341   0.1799  0.0440    0.0695\n",
       "Sensitivity  0.0000   0.7157  0.8127    0.8428\n",
       "Specificity  1.0000   0.8363  0.9782    0.9440\n",
       "Precision       NaN   0.4038  0.8526    0.7000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_NB = model_eval(NB_table1, model_name = 'NB') # Naive Bays, without oversampling\n",
    "Model_NB1 = model_eval(NB_table2, model_name = 'NB w OS') # Naive Bays, with oversampling\n",
    "Model_KNN = model_eval(KNN_table1, model_name = 'KNN') # K-Nearest Neighbor, without oversampling\n",
    "Model_KNN1 = model_eval(KNN_table2, model_name = 'KNN w OS') # K-Nearest Neighbor, with oversampling\n",
    "\n",
    "pd.concat([Model_NB, Model_NB1, Model_KNN, Model_KNN1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8671c52",
   "metadata": {},
   "source": [
    "#### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53872189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1]\n",
      "Valid mail \n",
      "mail: I don't think I can get away for a trek that long with family in town, sorry \n",
      "\n",
      "Valid mail \n",
      "mail: Congratulations ùó¨ùóºùòÇùóø $25,000.00 ùó¶ùó≤ùòÅùòÅùóπùó≤ùó∫ùó≤ùóªùòÅ ùóñùóµùó≤ùó∞ùó∏ ùóîùóøùóøùó∂ùòÉùó≤ùó± üëâüèª , ùóüùóÆùòÄùòÅ ùó±ùóÆùòÜ ùòÅùóº ùóñùóºùóªùó≥ùó∂ùóøùó∫ ‚úîWed, 27 Apr 2022 17:44:45 -0400__ZWs1O \n",
      "\n",
      "Spam mail \n",
      "mail: Congratulations ur awarded either √•¬£500 of CD gift vouchers & Free entry 2 our √•¬£100 weekly draw txt MUSIC to 87066 TnCs www.Ldew.com1win150ppmx3age16  \n",
      "\n",
      "Spam mail \n",
      "mail: URGENT! You have won a 1 week FREE membership in our ¬£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_mail=  pd.read_excel('Input Mail.xlsx')\n",
    "input_mail['clean_mail'] = input_mail['Input Mail'].apply(lambda x: finalpreprocess(x))\n",
    "\n",
    "X_test=input_mail['clean_mail']\n",
    "count1 = count_vect.transform(X_test)\n",
    "X_vector1 = tfidfTransformer.transform(count1)\n",
    "lsa_tfidf1 = lsa.transform(X_vector1)\n",
    "\n",
    "\n",
    "prediction = knn1.predict(lsa_tfidf1)\n",
    "print(prediction)\n",
    "    \n",
    "for i in range(0, input_mail.shape[0]): \n",
    "    if (prediction[i]==1):\n",
    "         print('Spam mail', '\\nmail:', input_mail['Input Mail'][i], '\\n')\n",
    "    else:\n",
    "         print('Valid mail', '\\nmail:', input_mail['Input Mail'][i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d494f4b",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">***************************** </h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
